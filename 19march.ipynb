{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cc1ea6-d800-4709-b1df-be9802cea67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in-maxing is a statistical technique for re-scaling numerical values into a [0,1] range. \\nFor example, a series of album ratings scaled from 70 to 150 could be min-maxed so that every rating falls\\non or between 0 and 1, and the proportional distance between data points is retained. \\nMin-maxing is a form of data normalization.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 \n",
    "\n",
    "\"\"\"in-maxing is a statistical technique for re-scaling numerical values into a [0,1] range. \n",
    "For example, a series of album ratings scaled from 70 to 150 could be min-maxed so that every rating falls\n",
    "on or between 0 and 1, and the proportional distance between data points is retained. \n",
    "Min-maxing is a form of data normalization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e21be2-be05-46bb-af1d-de85c6c9a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. \\nWhen dealing with features with hard boundaries, this is quite useful. For example,\\nwhen dealing with image data, the colors can range from only 0 to 255'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "\"\"\"Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. \n",
    "When dealing with features with hard boundaries, this is quite useful. For example,\n",
    "when dealing with image data, the colors can range from only 0 to 255\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5360b750-2bac-48c3-a955-7077f4769d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis, or PCA, is a dimensionality reduction method that is often used to \\nreduce the dimensionality of large data sets, by transforming a large set of variables into a smaller\\none that still contains most of the information in the large set.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3\n",
    "\"\"\"Principal component analysis, or PCA, is a dimensionality reduction method that is often used to \n",
    "reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller\n",
    "one that still contains most of the information in the large set.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed60fdf2-7357-4697-b30e-46a9df3fc7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used \\nfor feature extraction and dimensionality reduction. It aims to find the directions of maximum variance in\\nhigh-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "\"\"\"Principal component analysis (PCA) is an unsupervised linear transformation technique which is primarily used \n",
    "for feature extraction and dimensionality reduction. It aims to find the directions of maximum variance in\n",
    "high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca4166b6-a54c-441d-9d52-3637e60d699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 \n",
    "\n",
    "# for the food recommandation system we have the data sets of price , rating and deleviry time \n",
    "# all of the above features data are in different values there will be a lot of difference between the data and for making \n",
    "# the useful model we have make the data in one league for that we will use the min_max scalling so it will transform all \n",
    "# data in the range of 0 to 1\n",
    "# we have to import the library called Minmaxscaler from the sklearn \n",
    "# it will help to convert the data into the range of 0 to 1 \n",
    "# and the formula for the minmaxscalar is (x-xmean)/xmax - xmin\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a321493-8c2d-48cb-8f39-6cd00600f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 \n",
    "\n",
    "# for the stock price predictin model there would be multiple dimensions \n",
    "# ex = last price, quantity, current price , one week before price\n",
    "# there can be multiple dimensions but if we look at every feature the model can not give us the efficient prediction \n",
    "# for that we use the PCA for reduceing the dimensions of features \n",
    "# By Using the PCA we can convert it into 2 dimensions \n",
    "# and the data present in the line of the pc1 we can consider them as a feature\n",
    "# in this method there will be a loss of data but more of the important aspect would be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b25d19b5-5903-4a98-8ab4-39c8839fe95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "a = [1, 5, 10, 15, 20]\n",
    "min_max = MinMaxScaler(feature_range=(-1,1))\n",
    "min_max.fit_transform([[i] for i in a ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d064b7-7524-46ff-bb1b-8d9b0350cb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd96ded-4cde-4690-8193-c06c7d349f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
